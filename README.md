# Training small roberta model for Vietnamese with hugginface scripts
 - 6 layers, 12 attention heads and 384 hidden
 - Pre-tokenize using [underthesea word_tokenize](https://github.com/undertheseanlp/underthesea)
 - Using [Vietnews](https://github.com/ThanhChinhBK/vietnews) train dataset  and first 3gb of [vi Oscar-corpus](https://oscar-corpus.com/post/oscar-2019/).