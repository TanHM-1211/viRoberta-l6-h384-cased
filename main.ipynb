{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing and investigation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data (preprocess_data.py)\n",
    "- Data contains the first 3GB of Oscar-Corpus-VN ('./data/oscar-text-3g.txt') and ~0.5GB Vietnews (in './data/train_tokenized')\n",
    "- Pre-tokenize using nltk.word_tokenize and split first 3GB of Oscar-Corpus-VN (into './data/oscar-corpus')\n",
    "- Rename (from *.txt.seg to *.txt) and Reformat Vietnews data to line-by-line dataset: title-summary-body text-captions (of image)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFKC()\n",
    "])\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.post_processor = RobertaProcessing(sep=('</s>', 2), cls=('<s>', 0))\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "# special_tokens += [\"<unused{}>\".format(i) for i in range(50)]\n",
    "trainer = BpeTrainer(vocab_size=30005, special_tokens=special_tokens, min_frequency=10, show_progress=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_files = [os.path.join('data/vietnews', file) for file in os.listdir('data/vietnews')]\n",
    "train_files += [os.path.join('data/oscar-corpus', file) for file in sorted(os.listdir('data/oscar-corpus'))]\n",
    "for file in train_files:\n",
    "    if not os.path.isfile(file):\n",
    "        raise IsADirectoryError(file)\n",
    "\n",
    "tokenizer.train(train_files, trainer=trainer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer.save('./save/tokenizer.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from underthesea import word_tokenize as underthesea_word_tokenize\n",
    "\n",
    "def word_tokenize(text, format='text'):\n",
    "    return underthesea_word_tokenize(text, format=format)\n",
    "\n",
    "text = word_tokenize(\"Đây là một ví dụ khoong đảm bảo\", format='list')\n",
    "# text[3] = '<mask>'\n",
    "# text[0] = '<mask>'\n",
    "text = ' '.join([t.replace(' ', '_') for t in text])\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "tokenizer.decode(encoding.ids)\n",
    "# This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will be encoded differently whether it is at the beginning of the sentence (without space) or not\n",
    "# So it has space at the beginning to avoid that."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-05 15:20:28.010151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64\n",
      "2021-10-05 15:20:28.010185: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s>', 'ĠÄĲÃ¢y', 'ĠlÃł', 'Ġmá»Ļt', 'ĠvÃŃ', '_', 'dá»¥', 'Ġkho', 'ong', 'ĠÄĳáº£m', '_', 'báº£o', '</s>']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Đây là một ví_dụ khoong đảm_bảo'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# convert to Roberta tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_ = RobertaTokenizerFast.from_pretrained('./save')\n",
    "tokenizer_.save_pretrained('./save/viRoberta-l6-h384-word-cased/')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "file ./save/config.json not found\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./save/viRoberta-l6-h384-word-cased/tokenizer_config.json',\n",
       " './save/viRoberta-l6-h384-word-cased/special_tokens_map.json',\n",
       " './save/viRoberta-l6-h384-word-cased/vocab.json',\n",
       " './save/viRoberta-l6-h384-word-cased/merges.txt',\n",
       " './save/viRoberta-l6-h384-word-cased/added_tokens.json',\n",
       " './save/viRoberta-l6-h384-word-cased/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test scripts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizerFast, RobertaTokenizer, RobertaConfig\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./save/viRoberta-l6-h384-word-cased')\n",
    "tokenizer.model_max_length = 512\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "roberta_config = RobertaConfig()\n",
    "roberta_config.max_position_embeddings = 514\n",
    "roberta_config.hidden_act = \"gelu\"\n",
    "roberta_config.hidden_size = 384\n",
    "roberta_config.intermediate_size = 1536\n",
    "roberta_config.num_hidden_layers = 6\n",
    "roberta_config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "roberta = RobertaModel(roberta_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': [os.path.join('data/vietnews', file) for file in sorted(os.listdir('data/vietnews'))[:1000]],\n",
    "                                           }, streaming=True)\n",
    "# train_datasets = dataset['train'].shuffle(buffer_size=10000, seed=22)\n",
    "# val_datasets = dataset['val'].shuffle(buffer_size=10000, seed=22)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-26e92df873d26a89\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset text/default to /home/aimenext/.cache/huggingface/datasets/text/default-26e92df873d26a89/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fb67f20ce5f4889becedde7c6fe7fdd"
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81142b5a16c94b4a838fee68d16cf84a"
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8793c15ede904a8fbea25837aef704c1"
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset text downloaded and prepared to /home/aimenext/.cache/huggingface/datasets/text/default-26e92df873d26a89/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "825fe629ea79409287c1f48178bd97cc"
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# from transformers.data.data_collator import DataCollatorForWholeWordMask\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "data_collator = DataCollatorForWholeWordMask(tokenizer, mlm_probability=0.15)\n",
    "\n",
    "# approximate 5M sentences\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='save/checkpoint-viRoberta-l6-h384-word-cased',\n",
    "    do_train=True,\n",
    "    # do_eval=True,\n",
    "    # evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=64,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.1,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    seed=22,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=roberta,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset['train'],\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%tensorboard --logdir save/checkpoint-viRoberta-l6-h384-word-cased/runs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import pipeline, RobertaModel, RobertaTokenizer\n",
    "\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model = \"./save/checkpoint-viRoberta-l6-h384-word-cased\",\n",
    "    tokenizer=\"./save/checkpoint-viRoberta-l6-h384-word-cased\",\n",
    ")\n",
    "\n",
    "# The sun <mask>.\n",
    "# =>\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from underthesea import word_tokenize as underthesea_word_tokenize\n",
    "\n",
    "def word_tokenize(text, format='text'):\n",
    "    return underthesea_word_tokenize(text, format=format)\n",
    "\n",
    "text = word_tokenize(\"Xin chào, tôi không còn là sinh viên đại học Bách Khoa.\", format='text')\n",
    "text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Xin chào , tôi không còn là sinh_viên đại_học Bách_Khoa .'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "text = 'Xin chào , tôi không còn là <mask> đại_học Bách_Khoa .'\n",
    "fill_mask(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': 'Xin chào, tôi không còn là_ đại_học Bách_Khoa.',\n",
       "  'score': 0.17441324889659882,\n",
       "  'token': 67,\n",
       "  'token_str': '_'},\n",
       " {'sequence': 'Xin chào, tôi không còn là, đại_học Bách_Khoa.',\n",
       "  'score': 0.026248406618833542,\n",
       "  'token': 253,\n",
       "  'token_str': ','},\n",
       " {'sequence': 'Xin chào, tôi không còn là. đại_học Bách_Khoa.',\n",
       "  'score': 0.0072890338487923145,\n",
       "  'token': 264,\n",
       "  'token_str': '.'},\n",
       " {'sequence': 'Xin chào, tôi không còn là của đại_học Bách_Khoa.',\n",
       "  'score': 0.004515592474490404,\n",
       "  'token': 307,\n",
       "  'token_str': ' của'},\n",
       " {'sequence': 'Xin chào, tôi không còn là và đại_học Bách_Khoa.',\n",
       "  'score': 0.004377501085400581,\n",
       "  'token': 285,\n",
       "  'token_str': ' và'}]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('save/vietnews-checkpoint-viRoberta-l6-h384-word-cased')\n",
    "tokenizer.tokenize(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<mask>',\n",
       " 'Ġhá»įc',\n",
       " '_',\n",
       " 'phÃŃ',\n",
       " 'Ġtáº¡i',\n",
       " 'ĠBÃ¡ch',\n",
       " '_',\n",
       " 'Khoa',\n",
       " 'Ġcá»©',\n",
       " 'ĠtÄĥng',\n",
       " 'Ġ?']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "001563a95dc8b565e74778be5986e3895a5b5171a6836a45578c2beb3674daaa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}