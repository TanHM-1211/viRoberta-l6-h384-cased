{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 17:12:52.631422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/lib64\n",
      "2021-10-11 17:12:52.631468: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|██████████| 100/100 [00:00<00:00, 19491.17it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 17786.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from underthesea import sent_tokenize as sent_tokenize_uts, word_tokenize as word_tokenize_uts\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    return sent_tokenize_uts(doc)\n",
    "\n",
    "def word_tokenize(text, format='list'):\n",
    "    return word_tokenize_uts(text, format=format)\n",
    "\n",
    "\n",
    "def load_data_as_df(dir):\n",
    "    res = {\n",
    "      'name': [],\n",
    "      'title': [],\n",
    "      'summary': [],\n",
    "      'body': [],\n",
    "      'img_caption': [],\n",
    "    }\n",
    "    for file_name in tqdm(os.listdir(dir)[:100]):\n",
    "        file = os.path.join(dir, file_name)\n",
    "        try:\n",
    "            with open(file, encoding='utf-8') as f:\n",
    "                document = f.read().rstrip().split(\"\\n\\n\")\n",
    "            for i in range(len(document)):\n",
    "                document[i] = document[i].replace('\\n', ' ')\n",
    "                \n",
    "            if len(document) == 3:\n",
    "                title, summary, doc = document \n",
    "                img_caption = ''\n",
    "            elif len(document) == 4:\n",
    "                title, summary, doc, img_caption = document \n",
    "            res['name'].append(file_name)\n",
    "            res['title'] += [title]\n",
    "            res['summary'] += [summary]\n",
    "            res['body'] += [doc]\n",
    "            res['img_caption'] += [img_caption]\n",
    "        except:\n",
    "            continue\n",
    "      \n",
    "    return pd.DataFrame.from_dict(res)\n",
    "\n",
    "valid_data_df = load_data_as_df('../data/val_tokenized')\n",
    "train_data_df = load_data_as_df('../data/train_tokenized')\n",
    "\n",
    "valid_data = Dataset.from_pandas(valid_data_df)\n",
    "train_data = Dataset.from_pandas(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Zayt/viRoberta-l6-h384-word-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e50457781c4d2db0c15fd8b09f558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Mean: 729.34, %-Articles > 512:0.6, Summary Mean:52.42, %-Summary > 64:0.16, %-Summary > 128:0.0\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "def map_to_length(x):\n",
    "    x[\"article_len\"] = len(tokenizer(x[\"body\"]).input_ids)\n",
    "    x[\"article_longer_512\"] = int(x[\"article_len\"] > 512)\n",
    "    x[\"summary_len\"] = len(tokenizer(x[\"summary\"]).input_ids)\n",
    "    x[\"summary_longer_64\"] = int(x[\"summary_len\"] > 64)\n",
    "    x[\"summary_longer_128\"] = int(x[\"summary_len\"] > 128)\n",
    "    return x\n",
    "\n",
    "data_stats = train_data.select(range(sample_size)).map(map_to_length, num_proc=4)\n",
    "\n",
    "def compute_and_print_stats(x):\n",
    "  if len(x[\"article_len\"]) == sample_size:\n",
    "    print(\n",
    "        \"Article Mean: {}, %-Articles > 512:{}, Summary Mean:{}, %-Summary > 64:{}, %-Summary > 128:{}\".format(\n",
    "            sum(x[\"article_len\"]) / sample_size,\n",
    "            sum(x[\"article_longer_512\"]) / sample_size, \n",
    "            sum(x[\"summary_len\"]) / sample_size,\n",
    "            sum(x[\"summary_longer_64\"]) / sample_size,\n",
    "            sum(x[\"summary_longer_128\"]) / sample_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "output = data_stats.map(\n",
    "  compute_and_print_stats, \n",
    "  batched=True,\n",
    "  batch_size=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch, tokenizer=tokenizer):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"body\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f345aac1b34ab490d5dd871ab3a030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af216f003f44a168661e7a4878565d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# batch_size = 16\n",
    "batch_size=4\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"name\", 'title', 'summary', 'body', 'img_caption']\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "valid_data = valid_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"name\", 'title', 'summary', 'body', 'img_caption']\n",
    ")\n",
    "valid_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Zayt/viRoberta-l6-h384-word-cased were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at Zayt/viRoberta-l6-h384-word-cased and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at Zayt/viRoberta-l6-h384-word-cased and are newly initialized: ['roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"Zayt/viRoberta-l6-h384-word-cased\", \"Zayt/viRoberta-l6-h384-word-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n",
    "\n",
    "bert2bert.config.max_length = 130\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True, \n",
    "    output_dir=\"./save/checkpoint\",\n",
    "    logging_steps=2,\n",
    "    save_steps=15,\n",
    "    eval_steps=15,\n",
    "    # logging_steps=1000,\n",
    "    # save_steps=500,\n",
    "    # eval_steps=7500,\n",
    "    # warmup_steps=2000,\n",
    "    # save_total_limit=3,\n",
    ")\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\", \"rouge1\", \"rougeL\"])\n",
    "\n",
    "    return {\n",
    "        \"rouge1_fmeasure\": round(rouge_output['rouge1'].mid.fmeasure, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output['rouge2'].mid.fmeasure, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_output['rougeL'].mid.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/75 01:12 < 01:50, 0.40 it/s, Epoch 1.20/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1 Fmeasure</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "      <th>Rougel Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.909800</td>\n",
       "      <td>5.620352</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/25 00:09 < 01:06, 0.32 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./save/checkpoint/checkpoint-15\n",
      "Configuration saved in ./save/checkpoint/checkpoint-15/config.json\n",
      "Model weights saved in ./save/checkpoint/checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in ./save/checkpoint/checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in ./save/checkpoint/checkpoint-15/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17679/3495528496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_num_beams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     def predict(\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2110\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2113\u001b[0m         )\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2277\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m             \u001b[0;31m# Update containers on host\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         )\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# in case the batch is shorter than max length, the output should be padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             )\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/tan_hm/venv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1860\u001b[0m             \u001b[0mcur_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22644/22644 [00:01<00:00, 17315.08it/s]\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/vocab.json from cache at /mnt/disk1/.cache/torch/transformers/b0e30d4e65f5ace2978bfc0ac11349b3d7a6e99612a56d3c6a2b1b101a8c1bea.91e4c69b824b5019ca75d7df6f45ac3bc2a7c105ba0783921c380145ab741b43\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/merges.txt from cache at /mnt/disk1/.cache/torch/transformers/217d130a1f02d57e1ef004e1fa642b58048d74fae05139dffb70aaa250cd23f5.310710ce1c51d834d45706c163dd9bbbcebc1a68cfe7aefb469cc4eb810051fe\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/tokenizer.json from cache at /mnt/disk1/.cache/torch/transformers/4ffeb3a7a3adab541dec5435b618435910b86a0e5e575fa22cf057c3807080f0.c7aa177925a71126a79c08a0ae442213314561ffca32d063a268786ab2cca2b6\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/special_tokens_map.json from cache at /mnt/disk1/.cache/torch/transformers/4e3ea75429dc5660f2fda02b8b2cdaf1248d19dac39aa6c682cf39ad6ac61c90.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/Zayt/viRoberta-l6-h384-word-cased/resolve/main/tokenizer_config.json from cache at /mnt/disk1/.cache/torch/transformers/01fe2f6760379f9b23c2632263ea8d96591ff18b69d9f039cf82bf2b42936ddc.cea6c95359298125c7e169785376b83fee45934722d595fd46559155e69ceb27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c489077e9f9479ab374a05c124aa604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file save/checkpoint/checkpoint-9882/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"Zayt/viRoberta-l6-h384-word-cased\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"RobertaForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 384,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 1536,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 514,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"roberta\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 6,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.11.2\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 30005\n",
      "  },\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"Zayt/viRoberta-l6-h384-word-cased\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"RobertaForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 384,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 1536,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 514,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"roberta\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 6,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.11.2\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 30005\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 130,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 30005\n",
      "}\n",
      "\n",
      "loading weights file save/checkpoint/checkpoint-9882/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at save/checkpoint/checkpoint-9882.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 18:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Data preprocessing\n",
    "\n",
    "# %%\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(22)\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from underthesea import sent_tokenize as sent_tokenize_uts, word_tokenize as word_tokenize_uts\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    return sent_tokenize_uts(doc)\n",
    "\n",
    "def word_tokenize(text, format='list'):\n",
    "    return word_tokenize_uts(text, format=format)\n",
    "\n",
    "\n",
    "def load_data_as_df(dir):\n",
    "    res = {\n",
    "      'name': [],\n",
    "      'title': [],\n",
    "      'summary': [],\n",
    "      'body': [],\n",
    "      'img_caption': [],\n",
    "    }\n",
    "    for file_name in tqdm(os.listdir(dir)):\n",
    "        file = os.path.join(dir, file_name)\n",
    "        try:\n",
    "            with open(file, encoding='utf-8') as f:\n",
    "                document = f.read().rstrip().split(\"\\n\\n\")\n",
    "            for i in range(len(document)):\n",
    "                document[i] = document[i].replace('\\n', ' ')\n",
    "                \n",
    "            if len(document) == 3:\n",
    "                title, summary, doc = document \n",
    "                img_caption = ''\n",
    "            elif len(document) == 4:\n",
    "                title, summary, doc, img_caption = document \n",
    "            res['name'].append(file_name)\n",
    "            res['title'] += [title]\n",
    "            res['summary'] += [summary]\n",
    "            res['body'] += [doc]\n",
    "            res['img_caption'] += [img_caption]\n",
    "        except:\n",
    "            continue\n",
    "      \n",
    "    return pd.DataFrame.from_dict(res)\n",
    "\n",
    "test_data_df = load_data_as_df('../data/test_tokenized')\n",
    "\n",
    "# test_data = Dataset.from_pandas(test_data_df.sample(n=5000, random_state=222))\n",
    "test_data = Dataset.from_pandas(test_data_df)\n",
    "\n",
    "\n",
    "# %%\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Zayt/viRoberta-l6-h384-word-cased')\n",
    "\n",
    "\n",
    "# %%\n",
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch, tokenizer=tokenizer):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"body\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# %%\n",
    "# batch_size = 16\n",
    "batch_size=8\n",
    "\n",
    "test_data = test_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"name\", 'title', 'summary', 'body', 'img_caption']\n",
    ")\n",
    "test_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model\n",
    "\n",
    "# %%\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "\n",
    "# %%\n",
    "# bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"Zayt/viRoberta-l6-h384-word-cased\", \"Zayt/viRoberta-l6-h384-word-cased\")\n",
    "bert2bert = EncoderDecoderModel.from_pretrained('save/checkpoint/checkpoint-9882')\n",
    "\n",
    "\n",
    "# %%\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n",
    "\n",
    "bert2bert.config.max_length = 130\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4\n",
    "\n",
    "# %% [markdown]\n",
    "# # Training\n",
    "\n",
    "# %%\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=\"./save/test_result\",\n",
    "    per_device_eval_batch_size = 8,\n",
    "    dataloader_num_workers = 4,\n",
    "# device = 'cuda:1',\n",
    "    # do_train = True,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    warmup_ratio = 0.1,\n",
    "    seed = 22,\n",
    "    log_level='info',\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 50,\n",
    "    save_total_limit = 4,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\", \"rouge1\", \"rougeL\"])\n",
    "\n",
    "    return {\n",
    "        \"rouge1_fmeasure\": round(rouge_output['rouge1'].mid.fmeasure, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output['rouge2'].mid.fmeasure, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_output['rougeL'].mid.fmeasure, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# %%\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")\n",
    "eval_res = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.181546449661255,\n",
       " 'eval_rouge1_fmeasure': 0.5256,\n",
       " 'eval_rouge2_fmeasure': 0.1606,\n",
       " 'eval_rougeL_fmeasure': 0.2999,\n",
       " 'eval_runtime': 5183.5734,\n",
       " 'eval_samples_per_second': 4.368,\n",
       " 'eval_steps_per_second': 0.546}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.020521402359009,\n",
       " 'eval_rouge1_fmeasure': 0.5375,\n",
       " 'eval_rouge2_fmeasure': 0.1751,\n",
       " 'eval_rougeL_fmeasure': 0.3076,\n",
       " 'eval_runtime': 1105.5507,\n",
       " 'eval_samples_per_second': 4.523,\n",
       " 'eval_steps_per_second': 0.565}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62257e83c93fa3387442bd5aebc80bdf7ed58eaa472b333d9af121b22db2a637"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
